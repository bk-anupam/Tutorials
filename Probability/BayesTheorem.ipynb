{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes' Rule in Probability Theory <a id=\"bayes_rule\"></a>\n",
    "\n",
    "**Bayesâ€™ Rule** (or **Bayesâ€™ Theorem**) is a fundamental concept in probability theory and statistics that describes how to update the probability of a hypothesis based on new evidence. It provides a way to calculate a conditional probability by using prior knowledge and new data. This theorem is the backbone of **Bayesian inference** and allows for the continuous updating of beliefs in the presence of new information.\n",
    "\n",
    "### Formula of Bayes' Rule\n",
    "\n",
    "The mathematical form of **Bayes' Rule** is as follows:\n",
    "\n",
    "$$\n",
    "P(H|E) = \\frac{P(E|H) \\cdot P(H)}{P(E)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ P(H|E) $ is the **posterior probability**: the probability of the hypothesis $ H $ given the evidence $ E $ (what we're trying to calculate).\n",
    "- $ P(E|H) $ is the **likelihood**: the probability of observing the evidence $ E $ given that the hypothesis $ H $ is true.\n",
    "- $ P(H) $ is the **prior probability**: the probability of the hypothesis $ H $ before seeing the evidence (our initial belief about $ H $).\n",
    "- $ P(E) $ is the **marginal likelihood** (also called the **evidence**): the total probability of the evidence $ E $ under all possible hypotheses.\n",
    "\n",
    "### Explanation of the Terms\n",
    "\n",
    "1. **Prior Probability $ P(H) $**:\n",
    "   - The **prior** is the initial belief or probability about the hypothesis $ H $, before observing any data or evidence.\n",
    "   - For example, if you think a coin is fair before flipping it, you may assign a prior probability of 0.5 that it will land heads.\n",
    "\n",
    "2. **Likelihood $ P(E|H) $**:\n",
    "   - The **likelihood** represents how probable the evidence $ E $ is, assuming the hypothesis $ H $ is true.\n",
    "   - In the context of the coin flip, the likelihood might represent how likely it is to observe 7 heads out of 10 flips if you assume the coin is fair.\n",
    "\n",
    "3. **Posterior Probability $ P(H|E) $**:\n",
    "   - The **posterior** is the updated probability of the hypothesis $ H $, after taking into account the evidence $ E $.\n",
    "   - This is what Bayes' Rule helps to compute: it gives us a new probability estimate for the hypothesis after factoring in the new evidence.\n",
    "\n",
    "4. **Marginal Likelihood $ P(E) $**:\n",
    "   - The **marginal likelihood** (or **evidence**) is the total probability of observing the evidence $ E $, considering all possible hypotheses.\n",
    "   - This term normalizes the posterior probability, ensuring that the probabilities add up to 1. It can be computed as:\n",
    "     \n",
    "     $$\n",
    "     P(E) = \\sum_i P(E|H_i) \\cdot P(H_i)\n",
    "     $$\n",
    "   - In other words, itâ€™s the sum of the probabilities of the evidence occurring under each possible hypothesis.\n",
    "\n",
    "### Bayes' Rule in Words\n",
    "\n",
    "Bayes' Rule states that:\n",
    "\n",
    "- The **posterior probability** of a hypothesis $ H $ given the evidence $ E $ is proportional to the **likelihood** of the evidence given the hypothesis and the **prior** probability of the hypothesis.\n",
    "- You **update** your belief (prior) about a hypothesis based on how well it explains the observed data (likelihood).\n",
    "- The posterior is normalized by the **marginal likelihood**, which accounts for the overall probability of the evidence.\n",
    "\n",
    "### Example: Coin Flip\n",
    "\n",
    "Imagine you're testing whether a coin is biased or not (hypothesis $ H $). Initially, you believe the coin is fair with probability $ P(H) = 0.5 $ (this is your prior). You flip the coin 10 times and observe 7 heads. The question is: how should this evidence update your belief about the fairness of the coin?\n",
    "\n",
    "- **Prior** $ P(H) = 0.5 $: You initially think the coin is fair.\n",
    "- **Likelihood** $ P(E|H) $: You compute the probability of observing 7 heads out of 10 flips, assuming the coin is fair.\n",
    "- **Marginal likelihood** $ P(E) $: You consider all possible hypotheses (fair or biased coin) and calculate the total probability of observing 7 heads.\n",
    "- **Posterior** $ P(H|E) $: After observing 7 heads, you update your belief about whether the coin is fair or biased.\n",
    "\n",
    "By applying Bayes' Rule, you update your belief based on the observed evidence.\n",
    "\n",
    "### Intuitive Explanation of Bayes' Rule\n",
    "\n",
    "Bayesâ€™ Rule helps you **revise your prior beliefs** when new data or evidence becomes available. Hereâ€™s a simple breakdown:\n",
    "\n",
    "- **Start with your prior belief** (what you think before seeing any evidence).\n",
    "- **Check how well the evidence fits** with your hypothesis (this is the likelihood).\n",
    "- **Adjust your belief** based on how the evidence supports or contradicts your hypothesis.\n",
    "  \n",
    "If the evidence strongly supports your hypothesis, the posterior probability will increase; if the evidence contradicts your hypothesis, the posterior probability will decrease.\n",
    "\n",
    "### Use Cases of Bayes' Rule\n",
    "\n",
    "Bayesâ€™ Rule is widely used in various fields:\n",
    "- **Machine Learning**: In Bayesian inference models, like **Naive Bayes classifiers** and **Bayesian Neural Networks**.\n",
    "- **Medical Diagnosis**: Doctors use Bayesâ€™ Rule to update the probability of a disease based on symptoms and test results.\n",
    "- **Spam Filtering**: Email systems use Bayesian techniques to determine the likelihood that an email is spam based on certain keywords or characteristics.\n",
    "- **Decision Making**: Bayes' Rule is used to make informed decisions based on incomplete information.\n",
    "\n",
    "In summary, Bayes' Rule is a powerful tool in probability theory that provides a formal way to update beliefs or probabilities when new evidence is encountered. It helps refine our understanding of an uncertain world by combining prior knowledge with new data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Bayes' Rule in Spam Filtering: A Detailed Explanation**\n",
    "\n",
    "Spam filtering is a real-world application of **Bayes' Theorem**. Bayesian spam filters classify emails as **spam** or **not spam** based on the words they contain. This approach is known as **NaÃ¯ve Bayes Classification**.\n",
    "\n",
    "## **1. Understanding the Problem**\n",
    "We want to determine the probability that an email is spam ($S$) given that it contains certain words ($W$), i.e.,\n",
    "\n",
    "$$\n",
    "P(S | W)\n",
    "$$\n",
    "\n",
    "Using **Bayes' Theorem**, we rewrite this as:\n",
    "\n",
    "$$\n",
    "P(S | W) = \\frac{P(W | S) P(S)}{P(W)}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- **$P(S | W)$**: Probability that the email is spam **given the words in the email**.\n",
    "- **$P(W | S)$**: Probability of these words appearing in **spam emails**.\n",
    "- **$P(S)$**: Prior probability of an email being spam (percentage of spam emails in the dataset).\n",
    "- **$P(W)$**: Probability of the words appearing in **any email (spam or not)**.\n",
    "\n",
    "We also compute $P(\\neg S | W)$, the probability that an email is **not spam** given the words, and classify the email as spam if:\n",
    "\n",
    "$$\n",
    "P(S | W) > P(\\neg S | W)\n",
    "$$\n",
    "\n",
    "## **2. Step-by-Step Application of Bayes' Rule in Spam Filtering**\n",
    "\n",
    "### **Step 1: Building a Word Probability Database**\n",
    "A Bayesian spam filter requires a dataset of words commonly found in **spam** and **ham (not spam)** emails.\n",
    "\n",
    "- We collect a dataset of emails labeled as **spam** or **not spam**.\n",
    "- We count the occurrences of each word in both spam and ham emails.\n",
    "- We compute:\n",
    "  - $ P(W | S) $: Probability of a word appearing in spam emails.\n",
    "  - $ P(W | \\neg S) $: Probability of a word appearing in ham emails.\n",
    "\n",
    "Example dataset:\n",
    "\n",
    "| Word    | Spam Count | Ham Count |\n",
    "|---------|-----------|----------|\n",
    "| \"free\"  | 200       | 10       |\n",
    "| \"win\"   | 180       | 15       |\n",
    "| \"money\" | 160       | 5        |\n",
    "| \"hello\" | 10        | 300      |\n",
    "| \"meeting\" | 5       | 200      |\n",
    "\n",
    "We estimate probabilities like:\n",
    "\n",
    "$$\n",
    "P(\\text{\"free\"} | S) = \\frac{200}{\\text{Total Spam Words}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{\"free\"} | \\neg S) = \\frac{10}{\\text{Total Ham Words}}\n",
    "$$\n",
    "\n",
    "\n",
    "### **Step 2: Computing Prior Probabilities**\n",
    "We estimate:\n",
    "- **$P(S)$**: Probability that an email is spam (e.g., **40%** of all emails).\n",
    "- **$P(\\neg S)$**: Probability that an email is not spam (e.g., **60%** of all emails).\n",
    "\n",
    "### **Step 3: Applying Bayes' Rule to Classify an Email**\n",
    "Let's classify the email:\n",
    "\n",
    "```\n",
    "\"Win free money now!\"\n",
    "```\n",
    "\n",
    "We compute:\n",
    "\n",
    "$$\n",
    "P(S | W) = \\frac{P(W | S) P(S)}{P(W)}\n",
    "$$\n",
    "\n",
    "Using individual word probabilities:\n",
    "\n",
    "$$\n",
    "P(\\text{\"win\"} | S) = 180 / \\text{Total Spam Words}\n",
    "$$\n",
    "$$\n",
    "P(\\text{\"free\"} | S) = 200 / \\text{Total Spam Words}\n",
    "$$\n",
    "$$\n",
    "P(\\text{\"money\"} | S) = 160 / \\text{Total Spam Words}\n",
    "$$\n",
    "\n",
    "Since these words are **independent** (assumption in NaÃ¯ve Bayes), we multiply their probabilities:\n",
    "\n",
    "$$\n",
    "P(W | S) = P(\\text{\"win\"} | S) \\times P(\\text{\"free\"} | S) \\times P(\\text{\"money\"} | S)\n",
    "$$\n",
    "\n",
    "Similarly, we compute:\n",
    "\n",
    "$$\n",
    "P(W | \\neg S) = P(\\text{\"win\"} | \\neg S) \\times P(\\text{\"free\"} | \\neg S) \\times P(\\text{\"money\"} | \\neg S)\n",
    "$$\n",
    "\n",
    "Using Bayes' Theorem:\n",
    "\n",
    "$$\n",
    "P(S | W) = \\frac{P(W | S) P(S)}{P(W | S) P(S) + P(W | \\neg S) P(\\neg S)}\n",
    "$$\n",
    "\n",
    "If $ P(S | W) > 0.5 $, the email is classified as **spam**.\n",
    "\n",
    "## **3. Advantages of Bayesian Spam Filtering**\n",
    "âœ… **Self-learning**: The filter improves over time by updating probabilities.  \n",
    "âœ… **Lightweight**: Requires less computing power than deep learning models.  \n",
    "âœ… **Works on Any Language**: No language-specific rules needed.  \n",
    "âœ… **Handles New Spam Types**: Learns new spam words dynamically.  \n",
    "\n",
    "## **4. Limitations and Challenges**\n",
    "âŒ **Word Dependencies Ignored**: Assumes words are independent, which is not always true.  \n",
    "âŒ **Requires Training Data**: Needs good labeled spam and ham datasets.  \n",
    "âŒ **Spam Evasion Techniques**: Spammers use tricks like **misspellings** (e.g., \"fr\\u00a3\\u00a3 money\") or **invisible text** to bypass filters.  \n",
    "\n",
    "## **5. Enhancements to Bayesian Spam Filtering**\n",
    "ğŸ”¹ **Laplace Smoothing**: Avoids zero probability for unseen words.  \n",
    "ğŸ”¹ **Better Tokenization**: Handles numbers, special characters, and email structures.  \n",
    "ğŸ”¹ **Feature Engineering**: Uses email sender, links, and attachments.  \n",
    "ğŸ”¹ **Hybrid Models**: Combines Bayesian filtering with **machine learning (SVM, deep learning)** for better accuracy.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
