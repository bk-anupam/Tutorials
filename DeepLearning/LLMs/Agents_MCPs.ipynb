{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2b65c0d",
   "metadata": {},
   "source": [
    "**Question 1: What is model context protocol and how can I use it while building an agent using langgraph, langchain and google gemini 2.5 pro experimental.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7e85e5",
   "metadata": {},
   "source": [
    "### Gemini's answer:\n",
    "Okay, let's break down your questions regarding LangGraph, LangChain, Gemini 2.5 Pro Experimental, and tools.\n",
    "\n",
    "**1. Model Context Protocol with Gemini 2.5 Pro Experimental and LangGraph**\n",
    "\n",
    "* **Native Tool/Function Calling:** Gemini models, especially the newer ones like 2.5 Pro, support native function calling (also referred to as tool use). This is the core concept behind what some frameworks might call a \"Model Context Protocol\". The idea is that you describe the available tools (functions) to the model, and the model itself can decide when to call a specific tool and with what arguments, outputting a structured request for the tool call.\n",
    "* **LangChain Integration:** LangChain provides wrappers for models like Gemini (`ChatGoogleGenerativeAI`) that support this native function calling. You can define your tools using LangChain's standard methods (see next point) and then bind them to the model instance (often using the `.bind_tools()` or similar methods). When you invoke the model within your LangGraph agent, LangChain handles formatting the tool descriptions for Gemini, sending them with the prompt, and parsing Gemini's response if it requests a tool call.\n",
    "* **LangGraph Orchestration:** LangGraph is designed to orchestrate these interactions. A node in your graph might involve calling the Gemini model. If Gemini decides to use a tool, LangGraph routes the execution flow to another node responsible for running that specific tool, gets the result, and then typically feeds that result back to the Gemini model node for the next step.\n",
    "\n",
    "**Conclusion:** Yes, you can effectively use the *principles* of model-driven tool use (native function calling) with Gemini 2.5 Pro Experimental within a LangGraph agent. LangChain provides the necessary abstractions to define tools and integrate them with the Gemini model's native capabilities, and LangGraph orchestrates the resulting multi-step process.\n",
    "\n",
    "**2. Options for Creating New Tools for Your Agent**\n",
    "\n",
    "LangChain offers several flexible ways to create custom tools:\n",
    "\n",
    "1.  **The `@tool` Decorator (Recommended for Simplicity):**\n",
    "    * This is the easiest and most common method.\n",
    "    * Define a standard Python function that performs the desired action.\n",
    "    * Decorate it with `@tool` from `langchain_core.tools`.\n",
    "    * **Crucially, write a clear and descriptive docstring for the function.** This docstring is automatically used as the tool's description, which the LLM (Gemini) uses to understand *what the tool does*, *when to use it*, and *what arguments it expects*.\n",
    "    * LangChain automatically infers the tool's name (from the function name) and its arguments (from the function signature and type hints).\n",
    "    * Example:\n",
    "        ```python\n",
    "        from langchain_core.tools import tool\n",
    "\n",
    "        @tool\n",
    "        def get_weather(location: str) -> str:\n",
    "          \"\"\"Returns the current weather for a given location.\"\"\"\n",
    "          # ... (your code to fetch weather) ...\n",
    "          return f\"The weather in {location} is sunny.\"\n",
    "\n",
    "        @tool\n",
    "        def complex_calculation(x: float, y: float, operation: str = \"add\") -> float:\n",
    "          \"\"\"Performs a complex calculation (add, subtract, multiply) on two numbers x and y.\"\"\"\n",
    "          if operation == \"add\":\n",
    "            return x + y\n",
    "          elif operation == \"subtract\":\n",
    "            return x - y\n",
    "          # ... etc ...\n",
    "          return 0.0\n",
    "        ```\n",
    "\n",
    "2.  **Subclassing `BaseTool` (More Control):**\n",
    "    * For more complex scenarios where you need finer control over the tool's name, description, argument schema (using Pydantic), or how asynchronous execution is handled.\n",
    "    * You inherit from `langchain_core.tools.BaseTool` and must implement:\n",
    "        * `name`: A string identifier for the tool.\n",
    "        * `description`: A detailed string explaining the tool's purpose and usage.\n",
    "        * `args_schema`: A Pydantic `BaseModel` defining the expected input arguments and their types.\n",
    "        * `_run()`: The synchronous execution logic.\n",
    "        * `_arun()`: (Optional) The asynchronous execution logic.\n",
    "    * This gives you maximum flexibility but requires more boilerplate code.\n",
    "\n",
    "3.  **Using `StructuredTool.from_function()` (Schema Flexibility):**\n",
    "    * Similar to `@tool` but allows you to explicitly define the input schema using a Pydantic model if needed, separate from the function's signature.\n",
    "\n",
    "**Key Considerations When Creating Tools:**\n",
    "\n",
    "* **Descriptions are Vital:** The quality of your tool's description (usually from the docstring) is paramount. The LLM relies solely on this description to decide if and how to use the tool. Be clear, specific, and provide examples if necessary.\n",
    "* **Argument Schema:** Use type hints in your functions (`@tool`) or define a Pydantic `args_schema` (`BaseTool`) so the LLM knows exactly what arguments are needed and their types.\n",
    "* **Error Handling:** Implement robust error handling within your tool's logic.\n",
    "\n",
    "**3. Prebuilt Tools Available in Your Tech Stack (LangChain)**\n",
    "\n",
    "LangChain has a rich ecosystem of prebuilt tools and integrations, saving you from reinventing the wheel. Since LangGraph uses LangChain's tools, you have access to all of them. Here are some common categories and examples:\n",
    "\n",
    "* **Web Search:**\n",
    "    * `GoogleSearchRun` (via `GoogleSearchAPIWrapper`)\n",
    "    * `DuckDuckGoSearchRun`\n",
    "    * `TavilySearchResults` (often recommended for agent search)\n",
    "    * `BingSearchRun`\n",
    "    * `ArxivQueryRun`\n",
    "    * `PubMedQueryRun`\n",
    "* **Information Retrieval (RAG - Retrieval-Augmented Generation):**\n",
    "    * Tools built on top of Vector Stores (like Chroma, FAISS, Pinecone, etc.) using `create_retriever_tool`. These allow your agent to search through your own documents.\n",
    "* **Code Execution:**\n",
    "    * `PythonREPLTool` (executes Python code - use with caution!)\n",
    "    * Bash Process tools.\n",
    "* **Database Interaction:**\n",
    "    * `SQLDatabaseToolkit`: Allows querying SQL databases.\n",
    "* **API Interaction:**\n",
    "    * `RequestsTools`: Generic tools for making HTTP GET, POST, etc., requests.\n",
    "    * Specific API tools: Zapier, OpenWeatherMap, Wolfram Alpha, Wikipedia, various Google services (Drive, Calendar - often via `GoogleApiResourceTools`), GitHub, etc.\n",
    "* **File System:**\n",
    "    * Tools for reading, writing, and listing files.\n",
    "* **Toolkits:** Collections of related tools (e.g., `SQLDatabaseToolkit`, `CSVToolkit`, `JsonToolkit`).\n",
    "\n",
    "**Where to Find Them:**\n",
    "\n",
    "* **LangChain Documentation:** The best place to explore is the official LangChain documentation, specifically the \"Integrations\" -> \"Tools\" section.\n",
    "* **LangChain Community:** The ecosystem is constantly growing, with community contributions adding new tools frequently.\n",
    "\n",
    "When building your agent, start by checking if a prebuilt LangChain tool already exists for the functionality you need before creating a custom one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82222058",
   "metadata": {},
   "source": [
    "### Claude's Answer:\n",
    "Model context protocol refers to a standardized way of managing, passing, and accessing context information between different components in an AI system, especially in agent implementations. It's particularly relevant when building agents with frameworks like LangGraph and LangChain that interact with models like Google Gemini 2.5 Pro.\n",
    "\n",
    "Here's how you can use model context protocol when building an agent:\n",
    "\n",
    "1. **Context Management**\n",
    "   - Define what context your agent needs to track (conversation history, user preferences, tool outputs, etc.)\n",
    "   - Structure this context in a consistent format that can be efficiently passed between components\n",
    "\n",
    "2. **Implementation with LangGraph and LangChain**\n",
    "   - Use LangGraph's state management to maintain context between nodes in your agent graph\n",
    "   - Leverage LangChain's memory components to persist relevant information\n",
    "\n",
    "3. **Integration with Gemini 2.5 Pro**\n",
    "   - Format your context appropriately for Gemini's input requirements\n",
    "   - Use structured prompts that clearly delineate context information\n",
    "   - Consider context window limitations (Gemini 2.5 Pro Experimental has an expanded context window)\n",
    "\n",
    "A basic implementation might look like:\n",
    "\n",
    "```python\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langgraph.graph import StateGraph\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Initialize the model\n",
    "model = ChatGoogleGenerativeAI(model=\"models/gemini-2.5-pro-experimental\")\n",
    "\n",
    "# Create memory component for context\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "# Define state schema\n",
    "class AgentState(TypedDict):\n",
    "    context: dict  # Context protocol container\n",
    "    messages: list\n",
    "    next: str\n",
    "\n",
    "# Create state graph with context protocol\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "# Define nodes that maintain context\n",
    "def process_input(state):\n",
    "    # Extract context\n",
    "    current_context = state[\"context\"]\n",
    "    # Update context with new information\n",
    "    updated_context = {**current_context, \"last_input\": state[\"messages\"][-1].content}\n",
    "    # Return updated state\n",
    "    return {\"context\": updated_context, \"messages\": state[\"messages\"]}\n",
    "\n",
    "# Define reasoning node that uses context\n",
    "def reasoning(state):\n",
    "    context = state[\"context\"]\n",
    "    formatted_context = format_context_for_model(context)  # Convert to model-friendly format\n",
    "    \n",
    "    # Pass formatted context to model\n",
    "    response = model.invoke([\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful agent with the following context.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Context: {formatted_context}\\n\\nUser query: {state['messages'][-1].content}\"}\n",
    "    ])\n",
    "    \n",
    "    return {\"messages\": state[\"messages\"] + [response], \"next\": \"tool_selection\"}\n",
    "```\n",
    "\n",
    "When working specifically with Gemini 2.5 Pro Experimental, you might want to:\n",
    "\n",
    "1. Structure your context as JSON or other structured format that Gemini can easily parse\n",
    "2. Take advantage of Gemini's multimodal capabilities by including images or other data types in your context\n",
    "3. Utilize the expanded context window to maintain more comprehensive history\n",
    "\n",
    "Would you like more detailed information about any specific aspect of implementing model context protocol in your agent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491e2015",
   "metadata": {},
   "source": [
    "**Question 2: What do we mean when we say that we can build MCP servers for almost anything. How does an agent interact with a MCP server. How can I build an MCP server with using langgraph, langchain and google gemini 2.5 pro experimental.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03ddcf6",
   "metadata": {},
   "source": [
    "### Gemini's answer:\n",
    "Okay, let's clarify the term \"MCP Server\" in this context and how it relates to your agent architecture.\n",
    "\n",
    "As mentioned before, \"Model Context Protocol\" isn't a single standard protocol but a *pattern* for how models interact with tools. It seems you're using \"MCP Server\" to refer to an **external service, API, or application that exposes specific functionalities (tools) designed to be consumed by an LLM agent following this pattern.**\n",
    "\n",
    "It's not a standard industry term, so let's think of it as a **\"Tool Server\"** or **\"Agent-Action Server\"** – a backend service providing capabilities for your agent.\n",
    "\n",
    "**1. What does \"building MCP [Tool] servers for almost anything\" mean?**\n",
    "\n",
    "This means you can take virtually *any* piece of functionality, software capability, data source, or even hardware interaction and make it available to your LLM agent. You do this by:\n",
    "\n",
    "1.  **Encapsulating the Functionality:** Write code that performs the desired action (e.g., query a database, call a specific internal API, check inventory, control a smart device, perform a complex calculation, search a specific document repository).\n",
    "2.  **Exposing it via an API:** Create a web server (using frameworks like FastAPI, Flask, Node.js/Express, etc.) with specific API endpoints (URLs) that trigger this functionality. Define clear inputs (request parameters or body) and outputs (response format, often JSON).\n",
    "3.  **Describing it as a Tool:** Create a corresponding \"tool definition\" for your LLM agent (using LangChain's `@tool` or `BaseTool`). This definition tells the LLM:\n",
    "    * The tool's name (which might map to the API endpoint or a conceptual action).\n",
    "    * A clear description of *what the tool does* and *when to use it*.\n",
    "    * The required input parameters (which correspond to the API inputs).\n",
    "\n",
    "Essentially, you can build a bridge (the Tool Server/API) between the agent's \"brain\" (the LLM) and almost any capability you can programmatically access. The \"server\" is just the backend application hosting this capability and making it available over a network (usually HTTP).\n",
    "\n",
    "**2. How does an agent interact with an MCP [Tool] Server?**\n",
    "\n",
    "The interaction is orchestrated by your LangChain/LangGraph agent code, using the LLM's native function calling/tool use capability:\n",
    "\n",
    "1.  **User Request:** The user gives a prompt to the agent (e.g., \"What's the current stock level for product SKU 12345?\").\n",
    "2.  **LLM Decision:** The Gemini model, configured with a tool description like `get_stock_level(sku: str)` (which you defined in LangChain), analyzes the prompt. It recognizes that fulfilling the request requires using this tool and determines the `sku` is \"12345\".\n",
    "3.  **LLM Output (Tool Call):** Gemini outputs a structured message indicating a call to the `get_stock_level` tool with the argument `sku=\"12345\"`.\n",
    "4.  **LangGraph/LangChain Interception:** Your LangGraph agent receives this structured message. The `ToolExecutor` or your custom tool-handling node identifies the requested tool (`get_stock_level`).\n",
    "5.  **Tool Execution (API Call):** The *implementation* of your LangChain `get_stock_level` tool is executed. This implementation contains code (e.g., using the `requests` library) to:\n",
    "    * Construct an HTTP request (e.g., `GET http://your-tool-server.com/api/stock/12345`).\n",
    "    * Send this request to your running \"Tool Server\".\n",
    "6.  **Tool Server Processing:** Your external \"Tool Server\" (e.g., a FastAPI app) receives the request at its `/api/stock/{sku}` endpoint. It runs its internal logic (e.g., queries the inventory database for SKU 12345).\n",
    "7.  **Tool Server Response:** The Tool Server sends back an HTTP response containing the result (e.g., `{\"sku\": \"12345\", \"level\": 57}`).\n",
    "8.  **Result Processing:** The LangChain tool implementation receives the HTTP response, parses the stock level (57), and formats it as a `ToolMessage`.\n",
    "9.  **Feedback to LLM:** LangGraph routes this `ToolMessage` (containing \"Stock level is 57\") back to the Gemini model.\n",
    "10. **Final Response:** Gemini processes the tool's result and generates the final natural language response for the user (e.g., \"The current stock level for product SKU 12345 is 57 units.\").\n",
    "\n",
    "**Key Point:** The LLM *decides* to use the tool. The LangChain/LangGraph framework *orchestrates* the process. The LangChain *tool definition* contains the code to *actually call* the external Tool Server's API.\n",
    "\n",
    "**3. How can I build an MCP [Tool] Server with LangGraph, LangChain, and Gemini 2.5 Pro Experimental?**\n",
    "\n",
    "You use this stack to build the **agent** (the consumer), but you need a separate process/application for the **server** (the provider).\n",
    "\n",
    "**A. Building the Tool Server (Example using FastAPI):**\n",
    "\n",
    "This is a separate Python application. You *don't* typically run this inside your LangGraph agent process.\n",
    "\n",
    "```python\n",
    "# tool_server.py (Run this as a separate application: uvicorn tool_server:app --reload)\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import uvicorn # pip install fastapi uvicorn\n",
    "\n",
    "app = FastAPI(title=\"My Agent Tool Server\")\n",
    "\n",
    "# --- Example 1: Simple Data Lookup ---\n",
    "class ProductStock(BaseModel):\n",
    "    sku: str\n",
    "    level: int\n",
    "    status: str\n",
    "\n",
    "@app.get(\"/api/stock/{sku}\", response_model=ProductStock)\n",
    "async def get_stock_level_endpoint(sku: str):\n",
    "    \"\"\"Retrieves the stock level for a given product SKU.\"\"\"\n",
    "    print(f\"--- Tool Server: Received request for SKU: {sku} ---\")\n",
    "    # --- Replace with your actual database query/logic ---\n",
    "    await asyncio.sleep(0.5) # Simulate DB call\n",
    "    stock_level = 57 # Dummy data\n",
    "    # --- End Replace ---\n",
    "    return ProductStock(sku=sku, level=stock_level, status=\"in_stock\")\n",
    "\n",
    "# --- Example 2: Action ---\n",
    "class OrderDetails(BaseModel):\n",
    "    sku: str\n",
    "    quantity: int\n",
    "\n",
    "class OrderResult(BaseModel):\n",
    "    order_id: str\n",
    "    status: str\n",
    "    message: str\n",
    "\n",
    "@app.post(\"/api/orders\", response_model=OrderResult)\n",
    "async def place_order_endpoint(order: OrderDetails):\n",
    "    \"\"\"Places an order for a given product SKU and quantity.\"\"\"\n",
    "    print(f\"--- Tool Server: Received order for {order.quantity} of {order.sku} ---\")\n",
    "    # --- Replace with your actual order processing logic ---\n",
    "    await asyncio.sleep(1) # Simulate order processing\n",
    "    new_order_id = f\"ORD-{abs(hash(order.sku)) % 10000}\"\n",
    "    # --- End Replace ---\n",
    "    return OrderResult(order_id=new_order_id, status=\"accepted\", message=\"Order placed successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "     uvicorn.run(app, host=\"0.0.0.0\", port=8000) # Make accessible on local network if needed\n",
    "```\n",
    "\n",
    "**B. Building the Agent (LangGraph/LangChain/Gemini) to USE the Server:**\n",
    "\n",
    "This is where your main agent code lives.\n",
    "\n",
    "```python\n",
    "# agent.py (Your main LangGraph application)\n",
    "import requests # Or httpx for async\n",
    "from langchain_core.tools import tool\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.prebuilt import ToolExecutor\n",
    "from langgraph.graph import StateGraph, END\n",
    "# ... (import AgentState, BaseMessage, HumanMessage, ToolMessage etc. as before) ...\n",
    "\n",
    "# Define LangChain tools that CALL your Tool Server API endpoints\n",
    "TOOL_SERVER_BASE_URL = \"http://localhost:8000\" # Or the deployed URL of your server\n",
    "\n",
    "@tool\n",
    "def get_stock_level(sku: str) -> str:\n",
    "    \"\"\"Looks up the current stock level for a specific product SKU.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{TOOL_SERVER_BASE_URL}/api/stock/{sku}\")\n",
    "        response.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)\n",
    "        data = response.json()\n",
    "        return f\"Stock level for SKU {data['sku']} is {data['level']} ({data['status']}).\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error contacting stock server: {e}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error processing stock data: {e}\"\n",
    "\n",
    "@tool\n",
    "def place_order(sku: str, quantity: int) -> str:\n",
    "    \"\"\"Places an order for a given quantity of a product SKU.\"\"\"\n",
    "    try:\n",
    "        payload = {\"sku\": sku, \"quantity\": quantity}\n",
    "        response = requests.post(f\"{TOOL_SERVER_BASE_URL}/api/orders\", json=payload)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        return f\"Order status: {data['status']}. Order ID: {data['order_id']}. Message: {data['message']}\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error contacting order server: {e}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error processing order response: {e}\"\n",
    "\n",
    "# --- Agent Setup ---\n",
    "my_tools = [get_stock_level, place_order]\n",
    "tool_executor = ToolExecutor(my_tools)\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-pro-experimental-latest\") # Use your specific model ID\n",
    "llm_with_tools = llm.bind_tools(my_tools)\n",
    "\n",
    "# --- Graph Definition (Similar to previous examples) ---\n",
    "# ... (Define AgentState, call_model node using llm_with_tools, call_tool node using tool_executor)\n",
    "# ... (Define should_continue conditional edge logic)\n",
    "# ... (Build and compile the StateGraph) ...\n",
    "\n",
    "# --- Running the Agent ---\n",
    "# ... (Use app.stream or app.invoke with user input like \"How much stock of SKU XYZ is there?\")\n",
    "```\n",
    "\n",
    "**In Summary:**\n",
    "\n",
    "1.  You build a standard web server (like FastAPI) to host your custom logic/actions – this is your \"Tool Server\".\n",
    "2.  You build your agent using LangChain/LangGraph and Gemini.\n",
    "3.  Within the agent, you define LangChain tools where the implementation logic makes HTTP calls to your Tool Server's API endpoints.\n",
    "4.  Gemini decides *when* to use these tools based on their descriptions, and LangGraph orchestrates the process of calling the tool (which calls the server) and feeding the results back."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb56e10",
   "metadata": {},
   "source": [
    "### Claude's Answer:\n",
    "When we talk about building MCP (Model Context Protocol) servers for \"almost anything,\" we're referring to creating specialized services that manage context for different types of information or capabilities that an agent might need to access.\n",
    "\n",
    "## What is an MCP Server?\n",
    "\n",
    "An MCP server is essentially a dedicated service that:\n",
    "1. Stores, manages, and provides context relevant to a specific domain or function\n",
    "2. Presents this context in a standardized format that models can effectively use\n",
    "3. Can be queried or updated by agents as needed\n",
    "\n",
    "## How Agents Interact with MCP Servers\n",
    "\n",
    "The interaction flow typically works like this:\n",
    "\n",
    "1. **Context Request**: The agent sends a request to the MCP server when it needs specific contextual information\n",
    "2. **Context Retrieval**: The MCP server retrieves and formats the relevant context\n",
    "3. **Context Injection**: The formatted context is injected into the agent's prompt or reasoning process\n",
    "4. **Context Update**: After processing, the agent may send updates back to the MCP server\n",
    "\n",
    "This happens through API calls or direct integration depending on your architecture.\n",
    "\n",
    "## Building an MCP Server with LangGraph, LangChain, and Gemini 2.5 Pro\n",
    "\n",
    "Yes, you can definitely build an MCP server using these tools. Here's a conceptual approach:\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI, HTTPException, Depends\n",
    "from pydantic import BaseModel\n",
    "from typing import Dict, List, Any, Optional\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import GoogleGenerativeAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.prebuilt import ToolExecutor\n",
    "import uuid\n",
    "import json\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI(title=\"Model Context Protocol Server\")\n",
    "\n",
    "# Initialize the embedding model for vector storage\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatGoogleGenerativeAI(model=\"models/gemini-2.5-pro-experimental\")\n",
    "\n",
    "# Database to store different context types\n",
    "context_stores = {}\n",
    "\n",
    "class ContextRequest(BaseModel):\n",
    "    query: str\n",
    "    context_type: str\n",
    "    additional_params: Optional[Dict[str, Any]] = None\n",
    "\n",
    "class ContextUpdate(BaseModel):\n",
    "    context_type: str\n",
    "    data: Dict[str, Any]\n",
    "    metadata: Optional[Dict[str, Any]] = None\n",
    "\n",
    "class ContextResponse(BaseModel):\n",
    "    context_id: str\n",
    "    formatted_context: str\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "# Function to initialize a context store for a specific type\n",
    "def initialize_context_store(context_type: str):\n",
    "    if context_type not in context_stores:\n",
    "        # Create a vector store for this context type\n",
    "        context_stores[context_type] = {\n",
    "            \"vector_store\": Chroma(\n",
    "                collection_name=f\"mcp_{context_type}\",\n",
    "                embedding_function=embeddings,\n",
    "                persist_directory=f\"./data/{context_type}\"\n",
    "            ),\n",
    "            \"raw_data\": {}\n",
    "        }\n",
    "    return context_stores[context_type]\n",
    "\n",
    "# Define context processing for different context types\n",
    "def process_context(context_type: str, query: str, additional_params: Optional[Dict[str, Any]] = None):\n",
    "    # Get or initialize the store\n",
    "    store = initialize_context_store(context_type)\n",
    "    \n",
    "    # Define a context graph for processing\n",
    "    class ContextState(dict):\n",
    "        context_data: Dict[str, Any]\n",
    "        query: str\n",
    "        formatted_result: str\n",
    "    \n",
    "    # Create processing graph\n",
    "    context_graph = StateGraph(ContextState)\n",
    "    \n",
    "    # Define retrieval node\n",
    "    def retrieve_context(state):\n",
    "        # Retrieve relevant documents from vector store\n",
    "        docs = store[\"vector_store\"].similarity_search(\n",
    "            state[\"query\"], \n",
    "            k=5,\n",
    "            fetch_k=20\n",
    "        )\n",
    "        \n",
    "        # Get raw data for additional context\n",
    "        raw_data = {}\n",
    "        for doc_id in [doc.metadata.get(\"id\") for doc in docs]:\n",
    "            if doc_id in store[\"raw_data\"]:\n",
    "                raw_data[doc_id] = store[\"raw_data\"][doc_id]\n",
    "        \n",
    "        return {\"context_data\": {\"retrieved_docs\": [doc.page_content for doc in docs], \"raw_data\": raw_data}}\n",
    "    \n",
    "    # Define formatting node\n",
    "    def format_context(state):\n",
    "        # Use the LLM to format the context appropriately\n",
    "        prompt = f\"\"\"\n",
    "        You are formatting context for an AI agent. The agent needs context about: {context_type}\n",
    "        \n",
    "        The query is: {state[\"query\"]}\n",
    "        \n",
    "        Retrieved context information:\n",
    "        {json.dumps(state[\"context_data\"], indent=2)}\n",
    "        \n",
    "        Format this context in a clear, structured way that would be most useful for an AI agent.\n",
    "        Return ONLY the formatted context, nothing else.\n",
    "        \"\"\"\n",
    "        \n",
    "        response = llm.invoke(prompt)\n",
    "        formatted_context = response.content\n",
    "        \n",
    "        return {\"formatted_result\": formatted_context}\n",
    "    \n",
    "    # Add nodes to the graph\n",
    "    context_graph.add_node(\"retrieve\", retrieve_context)\n",
    "    context_graph.add_node(\"format\", format_context)\n",
    "    \n",
    "    # Add edges\n",
    "    context_graph.add_edge(\"retrieve\", \"format\")\n",
    "    context_graph.set_entry_point(\"retrieve\")\n",
    "    context_graph.set_finish_point(\"format\")\n",
    "    \n",
    "    # Compile and run the graph\n",
    "    app = context_graph.compile()\n",
    "    result = app.invoke({\n",
    "        \"query\": query,\n",
    "        \"context_data\": {},\n",
    "        \"formatted_result\": \"\"\n",
    "    })\n",
    "    \n",
    "    return result[\"formatted_result\"]\n",
    "\n",
    "# API endpoints\n",
    "@app.post(\"/context\", response_model=ContextResponse)\n",
    "async def get_context(request: ContextRequest):\n",
    "    try:\n",
    "        formatted_context = process_context(\n",
    "            request.context_type,\n",
    "            request.query,\n",
    "            request.additional_params\n",
    "        )\n",
    "        \n",
    "        context_id = str(uuid.uuid4())\n",
    "        \n",
    "        return ContextResponse(\n",
    "            context_id=context_id,\n",
    "            formatted_context=formatted_context,\n",
    "            metadata={\n",
    "                \"context_type\": request.context_type,\n",
    "                \"query\": request.query,\n",
    "                \"timestamp\": \"current_timestamp\"\n",
    "            }\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.post(\"/update\")\n",
    "async def update_context(update: ContextUpdate):\n",
    "    try:\n",
    "        store = initialize_context_store(update.context_type)\n",
    "        \n",
    "        # Generate an ID for this context\n",
    "        doc_id = str(uuid.uuid4())\n",
    "        \n",
    "        # Store the raw data\n",
    "        store[\"raw_data\"][doc_id] = update.data\n",
    "        \n",
    "        # Create a text representation for the vector store\n",
    "        text_representation = json.dumps(update.data)\n",
    "        \n",
    "        # Add to vector store\n",
    "        store[\"vector_store\"].add_texts(\n",
    "            texts=[text_representation],\n",
    "            metadatas=[{\"id\": doc_id, **update.metadata}] if update.metadata else [{\"id\": doc_id}]\n",
    "        )\n",
    "        \n",
    "        return {\"message\": \"Context updated successfully\", \"id\": doc_id}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\n",
    "```\n",
    "\n",
    "## Types of MCP Servers You Can Build\n",
    "\n",
    "You can build MCP servers for virtually any context type, such as:\n",
    "\n",
    "1. **Knowledge MCPs**: Provide domain-specific knowledge (medicine, law, finance)\n",
    "2. **Tool MCPs**: Manage context about available tools and how to use them\n",
    "3. **User MCPs**: Store user preferences, history, and personalization data\n",
    "4. **Reasoning MCPs**: Provide templates and examples for specific reasoning patterns\n",
    "5. **Planning MCPs**: Maintain context about plans, goals, and progress\n",
    "6. **Multimodal MCPs**: Handle contextual information across different modalities (text, image, audio)\n",
    "\n",
    "## Key Implementation Considerations\n",
    "\n",
    "When building your MCP server:\n",
    "\n",
    "1. **Structured Context Format**: Design a consistent format for your context\n",
    "2. **Context Relevance**: Implement effective retrieval to provide only relevant context\n",
    "3. **Stateful vs. Stateless**: Decide if your MCP server maintains state between requests\n",
    "4. **Context Window Management**: Format context efficiently to work within model token limits\n",
    "5. **Context Prioritization**: Develop mechanisms to prioritize the most important context\n",
    "6. **Security**: Implement appropriate access controls for sensitive context\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
