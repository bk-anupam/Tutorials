{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "\n",
    "### Introduction\n",
    "Batch Normalization and Layer Normalization are two powerful techniques that improve the performance and convergence of deep learning models. They work by rescaling the inputs to each layer, ensuring they have a mean of zero and a standard deviation of one. However, they differ in how they compute these statistics and their ideal use cases.\n",
    "\n",
    "### What is Batch Normalization?\n",
    "Batch Normalization (BatchNorm) computes the mean and variance of the inputs across the **batch dimension**. This helps:\n",
    "\n",
    "- Reduce the impact of different feature scales.\n",
    "- Speed up optimization by stabilizing gradients.\n",
    "- Reduce **internal covariate shift**, which refers to changes in input distribution during training.\n",
    "- Act as a **regularizer**, sometimes reducing the need for dropout.\n",
    "\n",
    "### How Batch Normalization Works\n",
    "#### **Step 1: Compute Mini-Batch Mean**\n",
    "For each feature in a mini-batch, compute the mean:\n",
    "\n",
    "$$ \\mu = \\frac{1}{m} \\sum_{i=1}^{m} z_i $$\n",
    "\n",
    "Where:\n",
    "- $ m $ is the number of samples in the batch.\n",
    "- $ z_i $ is the input feature.\n",
    "\n",
    "#### **Step 2: Compute Mini-Batch Variance**\n",
    "Compute the variance across the mini-batch:\n",
    "\n",
    "$$ \\sigma^2 = \\frac{1}{m} \\sum_{i=1}^{m} (z_i - \\mu)^2 $$\n",
    "\n",
    "#### **Step 3: Normalize Inputs**\n",
    "Normalize each feature in the mini-batch:\n",
    "\n",
    "$$ \\, \\hat{z}_i = \\frac{z_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} $$\n",
    "\n",
    "$ \\epsilon $ is a small constant added for numerical stability.\n",
    "\n",
    "#### **Step 4: Scale and Shift**\n",
    "Introduce two learnable parameters:\n",
    "- $ \\gamma $ (scale)\n",
    "- $ \\beta $ (shift)\n",
    "\n",
    "$$ z_{\\text{normalized}} = \\gamma \\hat{z}_i + \\beta $$\n",
    "\n",
    "These allow the model to learn optimal scaling and shifting for each feature.\n",
    "\n",
    "#### **Step 5: Forward Pass**\n",
    "Pass the normalized values through the layer and apply the activation function.\n",
    "\n",
    "#### **Step 6: Backpropagation**\n",
    "Compute gradients and update $ \\gamma $ and $ \\beta $ using an optimization algorithm like gradient descent.\n",
    "\n",
    "#### **Step 7: Repeat for Each Mini-Batch**\n",
    "Repeat the above steps for each mini-batch during training.\n",
    "\n",
    "### Key Takeaway\n",
    "Batch Normalization normalizes across the **batch dimension**, meaning it computes statistics over the batch for each feature.\n",
    "\n",
    "For an input of shape **(N, D)**, normalization is done over **N (batch size)**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Internal Covariate Shift \n",
    "\n",
    "refers to the phenomenon in which the distribution of the inputs to intermediate layers of a neural network changes as the network's parameters are updated during training. In other words, it's the change in the distribution of the activation values within a layer as the training progresses.\n",
    "\n",
    "When we're dealing with deep neural networks, especially those with many layers, the input distribution to each layer can vary significantly during the training process. This variance in input distributions can lead to slower convergence and make it challenging for the network to learn effectively.\n",
    "\n",
    "Internal Covariate Shift can have a negative impact on training for a few reasons:\n",
    "\n",
    "**1.Vanishing and Exploding Gradients:** As the distribution of inputs changes, gradients can become too small (vanishing gradients) or too large (exploding gradients), making it difficult to update the network's parameters effectively.\n",
    "\n",
    "**2.Slower Convergence:** When the input distribution changes, the optimization landscape also changes, potentially slowing down the convergence rate of the training algorithm.\n",
    "\n",
    "**3.Learning Rate Sensitivity:** Fluctuating input distributions can make choosing an appropriate learning rate more challenging, as a learning rate that's suitable for one distribution might not work well for another.\n",
    "\n",
    "To mitigate the negative effects of Internal Covariate Shift, techniques like Batch Normalization were introduced. Batch Normalization normalizes the inputs of each layer by using the mean and variance of the inputs within a mini-batch during training. This helps stabilize the distribution of inputs, making the training process more stable and efficient. Batch Normalization has become a common practice in modern deep learning architectures to address Internal Covariate Shift and facilitate the training of deeper networks.\n",
    "In summary, Internal Covariate Shift refers to the changing distribution of inputs within the layers of a deep neural network during training, which can hinder convergence and lead to issues like vanishing and exploding gradients. Batch Normalization is a technique used to counteract this phenomenon and improve the training process.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout: A Regularization Technique to Prevent Overfitting\n",
    "\n",
    "Dropout is a regularization technique used to prevent overfitting in neural networks by randomly dropping units (along with their connections) during training. This technique was introduced by Srivastava et al. in their 2014 paper: *\"Dropout: A Simple Way to Prevent Neural Networks from Overfitting.\"*\n",
    "\n",
    "### How Dropout Works\n",
    "1. **Randomly Dropping Units**\n",
    "   - During each training iteration, a fraction `p` (dropout rate) of the input units is randomly set to zero. This is done independently for each unit.\n",
    "\n",
    "2. **Scaling the Activations**\n",
    "   - To maintain the expected value of the outputs during training, the activations of the remaining units are scaled by `1 / (1 - p)`.\n",
    "\n",
    "3. **Different Network for Each Forward Pass**\n",
    "   - Since different units are dropped at each iteration, each forward pass effectively samples a different network. This results in an *implicit ensemble learning* effect, where multiple sub-networks are trained within the main network.\n",
    "\n",
    "4. **Inference (Testing Phase)**\n",
    "   - During inference, dropout is turned off, and the full network is used. The activations are scaled down by the dropout rate `p` to ensure consistency with the training phase.\n",
    "\n",
    "### Benefits of Dropout\n",
    "1. **Reduces Overfitting**\n",
    "   - Prevents the network from relying too heavily on any particular set of units, encouraging better generalization.\n",
    "\n",
    "2. **Promotes Redundancy**\n",
    "   - The network learns to distribute the representation of data across multiple units, making it more robust.\n",
    "\n",
    "3. **Implicit Ensemble Effect**\n",
    "   - Dropout effectively creates an ensemble of subnetworks that contribute to improved performance.\n",
    "\n",
    "### Important Considerations\n",
    "1. **Dropout Rate (`p`)**\n",
    "   - Common values for dropout rates are `0.2`, `0.5`, or `0.8`. The optimal value depends on the dataset and network architecture.\n",
    "\n",
    "2. **When to Use Dropout**\n",
    "   - Typically applied to fully connected (dense) layers.\n",
    "   - Can be applied to convolutional layers, though less commonly and usually with lower dropout rates.\n",
    "\n",
    "3. **Training vs. Inference**\n",
    "   - Dropout is applied only during training.\n",
    "   - During inference, all units are active, and activations are scaled appropriately.\n",
    "\n",
    "### Mathematical Formulation\n",
    "Let:\n",
    "\n",
    "- $z_i$ be the activation of unit i before dropout\n",
    "- $r_i$ be a binary random variable drawn from a Bernoulli distribution with probability **probability \n",
    "p of being 0** (meaning that fraction p of neurons are dropped) and **probability 1−p of being 1** (meaning that fraction 1−p of neurons remain active).\n",
    "\n",
    "During Training:\n",
    "$$\n",
    "\\tilde{z_i} = r_i \\cdot z_i\n",
    "$$\n",
    "$$\n",
    "a_i = \\frac{\\tilde{z_i}}{1 - p}\n",
    "$$\n",
    "During Inference:\n",
    "$$\n",
    "a_i = z_i\n",
    "$$\n",
    "This ensures that the expected value of activations remains consistent between training and inference.\n",
    "\n",
    "### Conclusion\n",
    "Dropout is a simple yet effective technique that prevents overfitting and enhances the generalization ability of neural networks. By randomly deactivating units, it forces the model to learn more robust features and effectively acts as an ensemble method.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing and Exploding Gradients\n",
    "\n",
    "Vanishing and exploding gradients are issues that can occur during the training of deep neural networks, particularly in architectures with many layers. These problems arise due to the way gradients are propagated backward through the network during the training process.\n",
    "\n",
    "\n",
    "### **Vanishing Gradients**\n",
    "Vanishing gradients occur when the gradients of the loss function with respect to the model's parameters become extremely small as they are propagated backward through the network layers. This means that the updates to the model's parameters become insignificant, leading to very slow or stagnant training.\n",
    "\n",
    "#### **Causes of Vanishing Gradients**\n",
    "- More prominent in deep networks with many layers.\n",
    "- Common when using activation functions that squash their inputs, such as the **Sigmoid** or **Tanh** functions.\n",
    "- Lower layers of the network receive very small gradients, resulting in minimal updates to parameters.\n",
    "- The network fails to learn meaningful features, leading to poor convergence.\n",
    "\n",
    "#### **Mathematical Explanation**\n",
    "For a given activation function $ f $:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial A} \\cdot \\frac{\\partial A}{\\partial Z} \\cdot \\frac{\\partial Z}{\\partial W}\n",
    "$$\n",
    "If $ \\frac{\\partial A}{\\partial Z} $ is very small (e.g., in sigmoid: $ \\sigma'(Z) = \\sigma(Z)(1 - \\sigma(Z)) $), then the gradients diminish exponentially as they propagate backward.\n",
    "\n",
    "\n",
    "### **Exploding Gradients**\n",
    "Exploding gradients occur when the gradients of the loss function become extremely large as they propagate backward through the layers. This can lead to unstable optimization and cause the network parameters to diverge rather than converge.\n",
    "\n",
    "#### **Causes of Exploding Gradients**\n",
    "- Happens when gradients are repeatedly multiplied by large weight values in deep networks.\n",
    "- Leads to extremely large updates to model parameters.\n",
    "- The optimization process becomes unstable, making it difficult to reach an optimal solution.\n",
    "\n",
    "#### **Mathematical Explanation**\n",
    "If the weight matrix $ W $ has large eigenvalues, repeated matrix multiplications lead to exponentially growing gradients:\n",
    "$$\n",
    "Z^{(l)} = W^{(l)} A^{(l-1)} + b^{(l)}\n",
    "$$\n",
    "When computing gradients:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} = \\prod_{l=L}^{1} W^{(l)}\n",
    "$$\n",
    "If $ W^{(l)} $ has large values, the gradients can grow exponentially, leading to instability.\n",
    "\n",
    "### **Mitigating Vanishing and Exploding Gradients**\n",
    "Several techniques have been developed to address these issues:\n",
    "\n",
    "#### **1. Activation Functions**\n",
    "- Use **ReLU (Rectified Linear Unit)** instead of Sigmoid or Tanh.\n",
    "- Variants like **Leaky ReLU** or **ELU** allow some gradient flow even for negative inputs.\n",
    "\n",
    "#### **2. Weight Initialization**\n",
    "- **Xavier/Glorot Initialization**: Suitable for activations like Sigmoid/Tanh.\n",
    "- **He Initialization**: Suitable for ReLU and its variants.\n",
    "\n",
    "#### **3. Batch Normalization**\n",
    "- Normalizes inputs at each layer to stabilize gradient flow.\n",
    "- Helps mitigate vanishing gradients by ensuring a stable distribution of activations.\n",
    "\n",
    "#### **4. Gradient Clipping**\n",
    "- Limits the gradients to a predefined threshold:\n",
    "```python\n",
    "import torch.nn.utils as utils\n",
    "utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
    "```\n",
    "- Prevents gradients from becoming too large and causing instability.\n",
    "\n",
    "#### **5. Skip Connections & Residual Networks (ResNets)**\n",
    "- Introduces shortcut connections that allow gradients to flow directly.\n",
    "- Example residual block:\n",
    "$$\n",
    "A^{(l+1)} = A^{(l)} + f(W^{(l)} A^{(l)} + b^{(l)})\n",
    "$$\n",
    "- Ensures that information is retained across layers.\n",
    "\n",
    "#### **6. Gradient Regularization**\n",
    "- **L2 Regularization (Weight Decay)**: Prevents large weight updates:\n",
    "```python\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "```\n",
    "- Helps prevent exploding gradients.\n",
    "\n",
    "#### **7. Specialized Architectures for RNNs**\n",
    "- **LSTM (Long Short-Term Memory)** and **GRU (Gated Recurrent Unit)** are designed to combat vanishing gradients in recurrent networks.\n",
    "- Uses gating mechanisms to control gradient flow.\n",
    "\n",
    "### **Conclusion**\n",
    "By implementing these techniques, we can effectively mitigate vanishing and exploding gradients, enabling the training of deeper and more complex neural networks. Understanding and addressing these issues is critical for building stable and well-performing deep learning models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer normalization \n",
    "\n",
    "Computes the mean and variance of the inputs across the feature dimension, which is the dimension that contains different features or channels. For example, if you have an input vector of size 128 for each sample, then layer normalization will compute the mean and variance using all 128 values for each sample. Layer normalization can help preserve the statistics of an individual sample, which can be important for some tasks such as natural language processing or generative modeling. Layer normalization is also independent of the batch size, so it can be applied to batches with smaller sizes or even single samples.\n",
    "Given an input x with dimensions [N,D] (where N is the batch size and D is the number of features), layer normalization computes the mean and variance for each data point or sample  (i.e., across the features):\n",
    "\n",
    "$$\n",
    "\\mu = \\frac{1}{D} \\sum_{i=1}^{D} x_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma^2 = \\frac{1}{D} \\sum_{i=1}^{D} (x_i - \\mu)^2\n",
    "$$\n",
    "\n",
    "Then, the normalized output z is computed as: \n",
    "$$\n",
    "z = \\gamma \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta\n",
    "$$\n",
    "\n",
    "where gamma and beta are learnable parameters that allow the model to scale and shift the normalized values, and epsilon is a small constant added for numerical stability. \n",
    "\n",
    "The choice between batch normalization and layer normalization depends on the type and distribution of your data, the architecture and objective of your model, and the size and variability of your batches. In general, batch normalization is more effective for convolutional neural networks that deal with images or other spatial data, while layer normalization is more effective for recurrent neural networks that deal with sequential data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
