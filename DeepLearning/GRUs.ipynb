{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **GRU (Gated Recurrent Unit)**\n",
    "\n",
    "A **Gated Recurrent Unit (GRU)** is a type of **Recurrent Neural Network (RNN)** designed to solve the vanishing gradient problem and improve long-term dependencies while keeping the computational cost lower than **LSTMs (Long Short-Term Memory networks).**\n",
    "\n",
    "### **Why GRUs?**\n",
    "\n",
    "Traditional RNNs suffer from:\n",
    "\n",
    "- **Vanishing gradient problem** → Cannot retain long-term dependencies.\n",
    "- **Exploding gradients** → Causes unstable training.\n",
    "- **Difficulty in learning long sequences**.\n",
    "\n",
    "GRUs solve these issues by introducing **gates** that control the flow of information, selectively deciding what information should be passed and what should be forgotten.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Structure of a GRU**\n",
    "\n",
    "A GRU consists of **two gates**:\n",
    "\n",
    "1. **Reset Gate ****$r_t$** → Decides how much past information to forget.\n",
    "2. **Update Gate ****$z_t$** → Controls how much of the new information should be used to update the hidden state.\n",
    "\n",
    "Unlike **LSTMs**, GRUs do not have a separate memory cell $C_t$, making them computationally more efficient.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Mathematical Formulation of GRU**\n",
    "\n",
    "At time step $t$, given:\n",
    "\n",
    "- **Input**: $x_t$ (current input)\n",
    "- **Previous hidden state**: $h_{t-1}$\n",
    "\n",
    "A GRU computes the next hidden state $h_t$ using the following steps:\n",
    "\n",
    "### **Step 1: Compute the Reset Gate**\n",
    "\n",
    "$$\n",
    "r_t = \\sigma(W_r x_t + U_r h_{t-1} + b_r)\n",
    "$$\n",
    "\n",
    "- $W_r$ and $U_r$ are weight matrices.\n",
    "- $b_r$ is the bias.\n",
    "- $\\sigma$ is the sigmoid activation function.\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "- If $r_t$ is close to **0**, it resets the previous hidden state, forgetting past information.\n",
    "- If $r_t$ is close to **1**, it keeps most of the past information.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Compute the Update Gate**\n",
    "\n",
    "$$\n",
    "z_t = \\sigma(W_z x_t + U_z h_{t-1} + b_z)\n",
    "$$\n",
    "\n",
    "- $W_z, U_z, b_z$ are the weights and bias.\n",
    "- $\\sigma$ is the sigmoid activation function.\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "- If $z_t$ is close to **0**, the hidden state is mostly influenced by the new candidate hidden state.\n",
    "- If $z_t$ is close to **1**, the hidden state remains mostly unchanged (helps in long-term memory retention).\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Compute the Candidate Hidden State**\n",
    "\n",
    "$$\n",
    "\\tilde{h}_t = \\tanh(W_h x_t + U_h (r_t \\odot h_{t-1}) + b_h)\n",
    "$$\n",
    "\n",
    "- $\\tilde{h}_t$ is the candidate hidden state.\n",
    "- $r_t \\odot h_{t-1}$ → Element-wise multiplication of reset gate and previous hidden state.\n",
    "- $\\tanh$ ensures values remain between $[-1,1]$.\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "- Uses $r_t$ to decide how much previous memory to include.\n",
    "- Helps the network selectively use relevant past information.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 4: Compute the Final Hidden State**\n",
    "\n",
    "$$\n",
    "h_t = (1 - z_t) \\odot \\tilde{h}_t + z_t \\odot h_{t-1}\n",
    "$$\n",
    "\n",
    "- If $z_t$ is **0**, it updates the hidden state fully using $\\tilde{h}_t$.\n",
    "- If $z_t$ is **1**, it keeps the old hidden state $h_{t-1}$.\n",
    "\n",
    "This equation ensures **smooth memory updates**, balancing new and old information.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. How GRUs Solve RNN Problems**\n",
    "\n",
    "- **Avoids vanishing gradients** by directly copying past hidden states when needed.\n",
    "- **Learns long-term dependencies** via the update gate.\n",
    "- **Simpler than LSTMs** (fewer parameters, faster training).\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Differences Between GRUs and LSTMs**\n",
    "\n",
    "| Feature                       | GRU                      | LSTM                               |\n",
    "| ----------------------------- | ------------------------ | ---------------------------------- |\n",
    "| Number of Gates               | 2 (Reset, Update)        | 3 (Forget, Input, Output)          |\n",
    "| Memory Cell                   | No separate memory cell  | Has a memory cell $C_t$          |\n",
    "| Computational Complexity      | Lower (fewer parameters) | Higher (more complex architecture) |\n",
    "| Performance on Small Datasets | Often better             | Requires more training data        |\n",
    "| Training Speed                | Faster                   | Slower due to more parameters      |\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Summary**\n",
    "\n",
    "- **GRUs** use **reset and update gates** to control memory flow.\n",
    "- **Mathematically**, they use **sigmoid and tanh activations** to regulate information retention.\n",
    "- **Compared to LSTMs**, they are **simpler and faster** while still handling long-term dependencies well. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
