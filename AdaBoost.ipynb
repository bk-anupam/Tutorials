{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of the AdaBoost Algorithm with Mathematics\n",
    "\n",
    "AdaBoost (Adaptive Boosting) is an ensemble learning method that combines multiple weak learners to create a strong learner. The algorithm adjusts the weights of training instances so that subsequent weak learners focus more on the misclassified instances.\n",
    "\n",
    "#### Initial Setup\n",
    "\n",
    "Initially, each training example $x_i$ is assigned a weight $w_i = \\frac{1}{N}$, where $N$ is the number of training examples.\n",
    "\n",
    "#### Training a Weak Learner\n",
    "\n",
    "For each iteration $t$:\n",
    "1. Train a weak learner $h_t(x)$ on the weighted training data.\n",
    "2. Calculate the error rate $\\epsilon_t$ of $h_t(x)$ with respect to the weights $w_i$:\n",
    "    $$\n",
    "    \\epsilon_t = \\frac{\\sum_{i=1}^N w_i I(y_i \\neq h_t(x_i))}{\\sum_{i=1}^N w_i}\n",
    "    $$\n",
    "    where $I(y_i \\neq h_t(x_i))$ is the indicator function that equals 1 if $y_i \\neq h_t(x_i)$ and 0 otherwise.\n",
    "\n",
    "#### Compute the Weight of the Weak Learner\n",
    "\n",
    "The weight $\\alpha_t$ of the weak learner $h_t$ is calculated using:\n",
    "$$\n",
    "\\alpha_t = \\frac{1}{2} \\ln \\left( \\frac{1 - \\epsilon_t}{\\epsilon_t} \\right)\n",
    "$$\n",
    "This weight reflects the confidence in the weak learner. A lower error $\\epsilon_t$ leads to a higher $\\alpha_t$, indicating higher confidence.\n",
    "\n",
    "#### Update the Weights of the Training Examples\n",
    "\n",
    "The weights $w_i$ of the training examples are updated as follows:\n",
    "$$\n",
    "w_i \\leftarrow w_i \\exp(-\\alpha_t y_i h_t(x_i))\n",
    "$$\n",
    "Here, $y_i$ is the true label of the example $x_i$, and $h_t(x_i)$ is the prediction of the weak learner for the example $x_i$. The term $\\exp(-\\alpha_t y_i h_t(x_i))$ adjusts the weight:\n",
    "- If the example $x_i$ is correctly classified ($y_i = h_t(x_i)$), then $y_i h_t(x_i) = 1$ and $\\exp(-\\alpha_t)$ is less than 1, so the weight $w_i$ is decreased.\n",
    "- If the example $x_i$ is misclassified ($y_i \\neq h_t(x_i)$), then $y_i h_t(x_i) = -1$ and $\\exp(\\alpha_t)$ is greater than 1, so the weight $w_i$ is increased.\n",
    "\n",
    "#### Normalization\n",
    "\n",
    "After updating the weights, normalize them so that they sum to 1:\n",
    "$$\n",
    "w_i \\leftarrow \\frac{w_i}{Z_t}\n",
    "$$\n",
    "where $Z_t$ is the normalization factor:\n",
    "$$\n",
    "Z_t = \\sum_{i=1}^N w_i \\exp(-\\alpha_t y_i h_t(x_i))\n",
    "$$\n",
    "\n",
    "### Why This Matters for Weight Updates\n",
    "\n",
    "The product $y_i h_t(x_i)$ helps determine the adjustment of the weights for the training examples:\n",
    "- If an example is correctly classified, $y_i h_t(x_i) = 1$, and its weight decreases.\n",
    "- If an example is misclassified, $y_i h_t(x_i) = -1$, and its weight increases.\n",
    "\n",
    "This mechanism ensures that the algorithm focuses more on the examples that are harder to classify in subsequent iterations, improving the overall performance of the ensemble model.\n",
    "\n",
    "### Summary\n",
    "\n",
    "In AdaBoost, the weight update step ensures that harder-to-classify examples receive more focus in subsequent iterations. The key steps involve calculating the error rate, determining the weight of the weak learner, updating the weights of the training examples, and normalizing the weights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Explanation of the Weight Update Step\n",
    "\n",
    "1. **Initial Setup**:\n",
    "   - At the start of the algorithm, each training example $ x_i $ is given an equal weight $ w_i = \\frac{1}{N} $, where $ N $ is the total number of training examples.\n",
    "\n",
    "2. **Training a Weak Learner**:\n",
    "   - Train a weak learner $ h_t(x) $ on the weighted training dataset.\n",
    "   - Calculate the error rate $ \\epsilon_t $ of the weak learner with respect to the current weights $ w_i $.\n",
    "\n",
    "3. **Compute the Weight of the Weak Learner**:\n",
    "   - The weight $ \\alpha_t $ of the weak learner is calculated using its error rate:\n",
    "     $\n",
    "     \\alpha_t = \\frac{1}{2} \\ln \\left( \\frac{1 - \\epsilon_t}{\\epsilon_t} \\right)\n",
    "     $\n",
    "   - This weight reflects the confidence in the weak learner. A lower error $ \\epsilon_t $ leads to a higher $ \\alpha_t $, indicating higher confidence.\n",
    "\n",
    "4. **Update the Weights of the Training Examples**:\n",
    "   - Update the weights $ w_i $ of the training examples based on whether they were correctly classified by the weak learner $ h_t $:\n",
    "     $\n",
    "     w_i \\leftarrow w_i \\exp(-\\alpha_t y_i h_t(x_i))\n",
    "     $\n",
    "   - Here, $ y_i $ is the true label of the example $ x_i $, and $ h_t(x_i) $ is the prediction of the weak learner for the example $ x_i $.\n",
    "   - The term $ \\exp(-\\alpha_t y_i h_t(x_i)) $ adjusts the weight as follows:\n",
    "     - If the example $ x_i $ is correctly classified ($ y_i = h_t(x_i) $), then $ y_i h_t(x_i) = 1 $ and $ \\exp(-\\alpha_t) $ is a value less than 1. Thus, the weight $ w_i $ is decreased.\n",
    "     - If the example $ x_i $ is misclassified ($ y_i \\neq h_t(x_i) $), then $ y_i h_t(x_i) = -1 $ and $ \\exp(\\alpha_t) $ is a value greater than 1. Thus, the weight $ w_i $ is increased.\n",
    "\n",
    "5. **Normalization**:\n",
    "   - After updating the weights, normalize them so that they sum to 1. This is done to ensure the weights form a valid probability distribution and to prevent numerical issues:\n",
    "     $\n",
    "     w_i \\leftarrow \\frac{w_i}{Z_t}\n",
    "     $\n",
    "   - $ Z_t $ is the normalization factor:\n",
    "     $\n",
    "     Z_t = \\sum_{i=1}^N w_i \\exp(-\\alpha_t y_i h_t(x_i))\n",
    "     $\n",
    "\n",
    "### Why This Matters for Weight Updates\n",
    "In the weight update formula, the term $ \\exp(-\\alpha_t y_i h_t(x_i)) $ is used to adjust the weights based on whether the prediction was correct or not:\n",
    "\n",
    "- If the example is correctly classified ($ y_i h_t(x_i) = 1 $), the update term becomes:\n",
    "  $\n",
    "  \\exp(-\\alpha_t \\cdot 1) = \\exp(-\\alpha_t)\n",
    "  $\n",
    "  Since $ \\alpha_t $ is positive, $ \\exp(-\\alpha_t) $ is less than 1, so the weight $ w_i $ decreases.\n",
    "\n",
    "- If the example is misclassified ($ y_i h_t(x_i) = -1 $), the update term becomes:\n",
    "  $\n",
    "  \\exp(-\\alpha_t \\cdot (-1)) = \\exp(\\alpha_t)\n",
    "  $\n",
    "  Since $ \\alpha_t $ is positive, $ \\exp(\\alpha_t) $ is greater than 1, so the weight $ w_i $ increases.\n",
    "\n",
    "### Summary\n",
    "In AdaBoost, the product $ y_i h_t(x_i) $ helps determine the adjustment of the weights for the training examples:\n",
    "- If an example is correctly classified, $ y_i h_t(x_i) = 1 $, and its weight decreases.\n",
    "- If an example is misclassified, $ y_i h_t(x_i) = -1 $, and its weight increases.\n",
    "\n",
    "This mechanism ensures that the algorithm focuses more on the examples that are harder to classify in subsequent iterations, improving the overall performance of the ensemble model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
