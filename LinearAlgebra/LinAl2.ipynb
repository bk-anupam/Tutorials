{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Outer Product](#outer_product)\n",
    "* [Inverse of a matrix](#inverse)\n",
    "* [Identity and Diagnol matrices](#identity)\n",
    "* [Rank](#rank)\n",
    "* [Orthonormal vectors](#orthonormal)\n",
    "* [Orthogonal matrix](#orthogonal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outer Product <a id=\"outer_product\"></a>\n",
    "\n",
    "The outer product of two vectors results in a matrix. Given two vectors $\\mathbf{u}$ and $\\mathbf{v}$, their outer product is a matrix $\\mathbf{A}$ where each element $A_{ij}$ is the product of the $i$-th element of $\\mathbf{u}$ and the $j$-th element of $\\mathbf{v}$. Formally, if $\\mathbf{u}$ is an $m$-dimensional vector and $\\mathbf{v}$ is an $n$-dimensional vector, then their outer product $\\mathbf{u} \\otimes \\mathbf{v}$ is an $m \\times n$ matrix defined as:\n",
    "\n",
    "$$ \\mathbf{A} = \\mathbf{u} \\otimes \\mathbf{v} $$\n",
    "$$ A_{ij} = u_i \\cdot v_j $$\n",
    "\n",
    "#### Example\n",
    "\n",
    "Let's consider two vectors:\n",
    "\n",
    "$$\n",
    "\\mathbf{u} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}, \\quad \\mathbf{v} = \\begin{bmatrix} 4 \\\\ 5 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The outer product $\\mathbf{u} \\otimes \\mathbf{v}$ is computed as follows:\n",
    "\n",
    "$$\n",
    "\\mathbf{A} = \\begin{bmatrix}\n",
    "1 \\cdot 4 & 1 \\cdot 5 \\\\\n",
    "2 \\cdot 4 & 2 \\cdot 5 \\\\\n",
    "3 \\cdot 4 & 3 \\cdot 5\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "4 & 5 \\\\\n",
    "8 & 10 \\\\\n",
    "12 & 15\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### Outer product of two matrices\n",
    "The outer product of two vectors results in a matrix, but the concept can be extended to the multiplication of two matrices. Matrix multiplication can be viewed as a series of outer products.\n",
    "\n",
    "Given two matrices $\\mathbf{A}$ of size $m \\times k$ and $\\mathbf{B}$ of size $k \\times n$, their product $\\mathbf{C}$ is a matrix of size $m \\times n$ defined as:\n",
    "\n",
    "$$ \\mathbf{C} = \\mathbf{A} \\cdot \\mathbf{B} $$\n",
    "\n",
    "Each element of $\\mathbf{C}$ is computed as:\n",
    "\n",
    "$$ C_{ij} = \\sum_{r=1}^{k} A_{ir} \\cdot B_{rj} $$\n",
    "\n",
    "Alternatively, matrix multiplication can be interpreted using the outer product. Specifically, we can decompose the multiplication into a sum of outer products of the columns of $\\mathbf{A}$ and the rows of $\\mathbf{B}$.\n",
    "\n",
    "Let's consider two matrices:\n",
    "\n",
    "$$\n",
    "\\mathbf{A} = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}, \\quad \\mathbf{B} = \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "To compute the product $\\mathbf{C} = \\mathbf{A} \\cdot \\mathbf{B}$ using outer products, we proceed as follows:\n",
    "\n",
    "1. Split $\\mathbf{A}$ into its columns and $\\mathbf{B}$ into its rows:\n",
    "\n",
    "$$\n",
    "\\mathbf{A}_1 = \\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix}, \\quad \\mathbf{A}_2 = \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{B}_1 = \\begin{bmatrix} 5 & 6 \\end{bmatrix}, \\quad \\mathbf{B}_2 = \\begin{bmatrix} 7 & 8 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "2. Compute the outer products:\n",
    "\n",
    "$$\n",
    "\\mathbf{A}_1 \\otimes \\mathbf{B}_1 = \\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix} \\otimes \\begin{bmatrix} 5 & 6 \\end{bmatrix} = \\begin{bmatrix} 5 & 6 \\\\ 15 & 18 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{A}_2 \\otimes \\mathbf{B}_2 = \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix} \\otimes \\begin{bmatrix} 7 & 8 \\end{bmatrix} = \\begin{bmatrix} 14 & 16 \\\\ 28 & 32 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "3. Sum the outer products to get $\\mathbf{C}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{C} = \\mathbf{A}_1 \\otimes \\mathbf{B}_1 + \\mathbf{A}_2 \\otimes \\mathbf{B}_2 = \\begin{bmatrix} 5 & 6 \\\\ 15 & 18 \\end{bmatrix} + \\begin{bmatrix} 14 & 16 \\\\ 28 & 32 \\end{bmatrix} = \\begin{bmatrix} 19 & 22 \\\\ 43 & 50 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Thus, the matrix product $\\mathbf{C}$ is:\n",
    "\n",
    "$$\n",
    "\\mathbf{C} = \\begin{bmatrix} 19 & 22 \\\\ 43 & 50 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### Interpretation and Applications\n",
    "\n",
    "- **Matrix Decomposition**: Viewing matrix multiplication as a sum of outer products is useful in matrix decomposition techniques like Singular Value Decomposition (SVD).\n",
    "- **Parallel Computation**: Breaking down matrix multiplication into sums of outer products can be advantageous in parallel computing environments.\n",
    "- **Tensor Products**: In higher-dimensional spaces, the concept extends to tensor products, useful in various applications in physics and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse of a Matrix <a id=\"inverse\"></a>\n",
    "\n",
    "The inverse of a matrix is a matrix that, when multiplied by the original matrix, yields the identity matrix. Not all matrices have inverses; a matrix must be square (same number of rows and columns) and have a non-zero determinant to have an inverse. We say that a matrix is **invertible** or **non-singlular** if it has an inverse and **non-invertible** or **singular** otherwise.\n",
    "\n",
    "Given a square matrix $\\mathbf{A}$, its inverse is denoted as $\\mathbf{A}^{-1}$. The defining property of the inverse is:\n",
    "\n",
    "$$\n",
    "\\mathbf{A} \\cdot \\mathbf{A}^{-1} = \\mathbf{A}^{-1} \\cdot \\mathbf{A} = \\mathbf{I}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{I}$ is the identity matrix of the same dimension as $\\mathbf{A}$.\n",
    "\n",
    "<u>Conditions for Invertibility</u>\n",
    "\n",
    "1. **Square Matrix**: The matrix must be square.\n",
    "2. **Non-zero Determinant**: The determinant of the matrix must be non-zero ($\\text{det}(\\mathbf{A}) \\neq 0$).\n",
    "\n",
    "<u>How to Compute the Inverse</u>\n",
    "\n",
    "For a $2 \\times 2$ matrix:\n",
    "\n",
    "$$\n",
    "\\mathbf{A} = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The inverse $\\mathbf{A}^{-1}$ is given by:\n",
    "\n",
    "$$\n",
    "\\mathbf{A}^{-1} = \\frac{1}{ad - bc} \\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $ad - bc$ is the determinant of $\\mathbf{A}$.\n",
    "\n",
    "For larger matrices, the inverse can be computed using various methods, such as Gaussian elimination, the adjoint method, or using matrix decomposition techniques.\n",
    "\n",
    "##### Example\n",
    "\n",
    "Let's find the inverse of the following $2 \\times 2$ matrix:\n",
    "\n",
    "$$\n",
    "\\mathbf{A} = \\begin{bmatrix} 2 & 3 \\\\ 1 & 4 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "First, compute the determinant:\n",
    "\n",
    "$$\n",
    "\\text{det}(\\mathbf{A}) = (2 \\cdot 4) - (3 \\cdot 1) = 8 - 3 = 5\n",
    "$$\n",
    "\n",
    "Since the determinant is non-zero, the inverse exists. Now, compute the inverse:\n",
    "\n",
    "$$\n",
    "\\mathbf{A}^{-1} = \\frac{1}{5} \\begin{bmatrix} 4 & -3 \\\\ -1 & 2 \\end{bmatrix} = \\begin{bmatrix} \\frac{4}{5} & -\\frac{3}{5} \\\\ -\\frac{1}{5} & \\frac{2}{5} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "<u> Properties of the Inverse </u>\n",
    "1. **Uniqueness**:\n",
    "   - The inverse of a matrix, if it exists, is unique. This means that there is only one matrix $\\mathbf{A}^{-1}$ that satisfies the equation $\\mathbf{A} \\mathbf{A}^{-1} = \\mathbf{I}$.\n",
    "\n",
    "2. **Invertibility Condition**:\n",
    "   - A matrix $\\mathbf{A}$ is invertible (non-singular) if and only if $\\text{det}(\\mathbf{A}) \\neq 0$. If $\\text{det}(\\mathbf{A}) = 0$, then $\\mathbf{A}$ is singular and does not have an inverse.\n",
    "\n",
    "3. **Inverse of a Product**:\n",
    "   - The inverse of a product of two invertible matrices $\\mathbf{A}$ and $\\mathbf{B}$ is given by:\n",
    "     $$\n",
    "     (\\mathbf{A} \\mathbf{B})^{-1} = \\mathbf{B}^{-1} \\mathbf{A}^{-1}\n",
    "     $$\n",
    "     Note that the order of multiplication is reversed.\n",
    "\n",
    "4. **Inverse of a Transpose**:\n",
    "   - The inverse of the transpose of an invertible matrix $\\mathbf{A}$ is the transpose of the inverse of $\\mathbf{A}$:\n",
    "     $$\n",
    "     (\\mathbf{A}^\\top)^{-1} = (\\mathbf{A}^{-1})^\\top\n",
    "     $$\n",
    "\n",
    "5. **Inverse of an Inverse**:\n",
    "   - The inverse of the inverse of a matrix $\\mathbf{A}$ is the matrix itself:\n",
    "     $$\n",
    "     (\\mathbf{A}^{-1})^{-1} = \\mathbf{A}\n",
    "     $$\n",
    "   \n",
    "6. **Inverse of a Diagonal Matrix**:\n",
    "   - If $\\mathbf{D}$ is a diagonal matrix with non-zero entries $d_i$ on the diagonal, the inverse of $\\mathbf{D}$ is a diagonal matrix with entries $1/d_i$ on the diagonal:\n",
    "     $$\n",
    "     \\mathbf{D}^{-1} = \\text{diag}(1/d_1, 1/d_2, \\ldots, 1/d_n)\n",
    "     $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identity and Diagonal Matrices <a id=\"identity\"></a>\n",
    "\n",
    "#### Identity Matrix\n",
    "\n",
    "An identity matrix, denoted by $\\mathbf{I}$ or $\\mathbf{I}_n$ for an $n \\times n$ matrix, is a square matrix where all elements on the main diagonal (from the top left to the bottom right) are 1, and all other elements are 0:\n",
    "\n",
    "$$\n",
    "\\mathbf{I}_n = \\begin{bmatrix} \n",
    "1 & 0 & \\cdots & 0 \\\\ \n",
    "0 & 1 & \\cdots & 0 \\\\ \n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\ \n",
    "0 & 0 & \\cdots & 1 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "<u> Properties </u>\n",
    "1. **Identity Property**: For any matrix $\\mathbf{A}$ of appropriate size,\n",
    "   $$\n",
    "   \\mathbf{A} \\cdot \\mathbf{I} = \\mathbf{I} \\cdot \\mathbf{A} = \\mathbf{A}\n",
    "   $$\n",
    "   This property states that multiplying any matrix by the identity matrix leaves the matrix unchanged.\n",
    "\n",
    "2. **Inverse Property**: The identity matrix is its own inverse, i.e.,\n",
    "   $$\n",
    "   \\mathbf{I}^{-1} = \\mathbf{I}\n",
    "   $$\n",
    "   This is because multiplying $\\mathbf{I}$ by itself results in $\\mathbf{I}$.\n",
    "\n",
    "3. **Diagonal Elements**: The diagonal elements are all 1, i.e., $I_{ii} = 1$ for all $i$.\n",
    "\n",
    "#### Diagonal Matrix\n",
    "\n",
    "A diagonal matrix is a square matrix where all elements off the main diagonal (non-diagonal elements) are zero. Formally, for a diagonal matrix $\\mathbf{D}$ of size $n \\times n$,\n",
    "$$\n",
    "D_{ij} = 0 \\quad \\text{for all} \\quad i \\neq j\n",
    "$$\n",
    "The diagonal elements $D_{ii}$ can be any real or complex numbers.\n",
    "\n",
    "<u> Properties </u>\n",
    "1. **Multiplication Property**: Diagonal matrices commute under multiplication, meaning for any two diagonal matrices $\\mathbf{D}$ and $\\mathbf{E}$,\n",
    "   $$\n",
    "   \\mathbf{D} \\cdot \\mathbf{E} = \\mathbf{E} \\cdot \\mathbf{D}\n",
    "   $$\n",
    "   This property follows because non-diagonal elements are zero.\n",
    "\n",
    "2. **Eigenvalues**: The eigenvalues of a diagonal matrix are simply its diagonal elements.\n",
    "\n",
    "3. **Inverse (if exists)**: A diagonal matrix is invertible (has an inverse) if and only if all diagonal elements are non-zero. If invertible, the inverse matrix $\\mathbf{D}^{-1}$ has its diagonal elements as reciprocals of the corresponding elements in $\\mathbf{D}$.\n",
    "\n",
    "#### Identity Matrix Example\n",
    "\n",
    "For a $3 \\times 3$ identity matrix:\n",
    "$$\n",
    "\\mathbf{I}_3 = \\begin{bmatrix} \n",
    "1 & 0 & 0 \\\\ \n",
    "0 & 1 & 0 \\\\ \n",
    "0 & 0 & 1 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### Diagonal Matrix Example\n",
    "\n",
    "For a $3 \\times 3$ diagonal matrix:\n",
    "$$\n",
    "\\mathbf{D} = \\begin{bmatrix} \n",
    "2 & 0 & 0 \\\\ \n",
    "0 & -1 & 0 \\\\ \n",
    "0 & 0 & 4 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### Verification of Properties:\n",
    "- **Identity Matrix**: Verify multiplication and inverse properties.\n",
    "- **Diagonal Matrix**: Verify commutative property and eigenvalues.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank of a Matrix <a id=\"rank\"></a>\n",
    "\n",
    "The **rank** of a matrix $\\mathbf{A}$, denoted as $\\text{rank}(\\mathbf{A})$, is the maximum number of linearly independent rows (or columns) in $\\mathbf{A}$.\n",
    "\n",
    "- **Definition**: For an $m \\times n$ matrix $\\mathbf{A}$, the rank is defined as:\n",
    "  - The dimension of the column space (range) of $\\mathbf{A}$.\n",
    "  - The dimension of the row space of $\\mathbf{A}$.\n",
    "  - The number of linearly independent rows or columns of $\\mathbf{A}$.\n",
    "\n",
    "<u> Properties of the Rank of a Matrix </u>\n",
    "\n",
    "1. **Column Rank Equals Row Rank**: For any matrix $\\mathbf{A}$, $\\text{rank}(\\mathbf{A})$ is equal to the number of linearly independent rows or columns. Therefore, $\\text{rank}(\\mathbf{A}) = \\text{rank}(\\mathbf{A}^\\top)$.\n",
    "\n",
    "2. **Maximum Number of Linearly Independent Rows or Columns**: The rank of $\\mathbf{A}$ is the largest number of linearly independent rows or columns that can be selected from $\\mathbf{A}$. This property is crucial in determining the dimension of the space spanned by the rows or columns of $\\mathbf{A}$.\n",
    "\n",
    "3. **Rank and Inverse**: If $\\mathbf{A}$ is a square matrix ($n \\times n$) and $\\mathbf{A}$ is invertible, then $\\text{rank}(\\mathbf{A}) = n$. Conversely, if $\\mathbf{A}$ is not invertible, then $\\text{rank}(\\mathbf{A}) < n$.\n",
    "\n",
    "4. **Rank and Eigenvalues**: The rank of a matrix $\\mathbf{A}$ is also related to its eigenvalues. Specifically, the number of non-zero eigenvalues of $\\mathbf{A}$ equals $\\text{rank}(\\mathbf{A})$.\n",
    "\n",
    "5. Rank($\\mathbf{A}^\\top$) = Rank($\\mathbf{A}$) for $\\mathbf{A} \\in \\mathbb{R}^{m,n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Orthonormal vectors <a id=\"orthonormal\"></a>\n",
    "In linear algebra, orthonormal vectors are a special type of set of vectors that possess two key properties:\n",
    "- **Orthogonality:** Each pair of vectors in the set is perpendicular to each other. This means the dot product (also called the scalar product) between any two distinct vectors in the set is zero. Mathematically, for vectors u and v in the set: u ⋅ v = 0 (where ⋅ denotes the dot product)\n",
    "- **Unit Norm:** Every vector in the set has a magnitude (or length) of 1. This is also referred to as being a unit vector. The norm of a vector is typically denoted by ||u||. Therefore, for any vector u in the set: ||u|| = 1\n",
    "\n",
    "Benefits of Orthonormal Sets:\n",
    "\n",
    "Orthonormal sets offer several advantages in various mathematical applications:\n",
    "- **Simplicity in Calculations:** Because the dot product between any two distinct vectors is zero, calculations involving these vectors can be simplified. For example, projections of vectors onto other vectors within the set become more straightforward.\n",
    "- **Basis for Vector Spaces:** An orthonormal set can be used as a basis for a vector space. **A basis is a set of linearly independent vectors that can span the entire space.** Orthonormal bases have specific advantages in areas like coordinate systems and projections.\n",
    "\n",
    "The standard basis vectors in Euclidean space (i.e., (1,0,0), (0,1,0), and (0,0,1) for 3D space) are an example of an orthonormal set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orthogonal matrix <a id=\"orthogonal\"></a>\n",
    "\n",
    "An orthogonal matrix is a square matrix $ \\mathbf{Q} $ whose columns and rows are orthonormal vectors. This means that:\n",
    "\n",
    "1. The columns of $ \\mathbf{Q} $ are orthonormal.\n",
    "2. The rows of $ \\mathbf{Q} $ are orthonormal.\n",
    "\n",
    "A matrix is orthogonal if and only if its transpose is equal to its inverse i.e. $ \\mathbf{Q}^T = \\mathbf{Q}^{-1} $. Using the property of matrix inverse it then follows that: \n",
    "\n",
    "$$ \\mathbf{Q}^T \\mathbf{Q} = \\mathbf{Q} \\mathbf{Q}^T = \\mathbf{I} $$\n",
    "\n",
    "where $ \\mathbf{Q}^T $ is the transpose of $ \\mathbf{Q} $ and $ \\mathbf{I} $ is the identity matrix.\n",
    "\n",
    "#### Example\n",
    "\n",
    "Consider the matrix:\n",
    "\n",
    "$$ \\mathbf{Q} = \\begin{pmatrix}\n",
    "\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\\\\n",
    "\\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}}\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "To check if $ \\mathbf{Q} $ is orthogonal, we need to verify that $ \\mathbf{Q}^T \\mathbf{Q} = \\mathbf{I} $.\n",
    "\n",
    "1. Compute the transpose of $ \\mathbf{Q} $:\n",
    "\n",
    "$$ \\mathbf{Q}^T = \\begin{pmatrix}\n",
    "\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\\\\n",
    "\\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}}\n",
    "\\end{pmatrix}^T = \\begin{pmatrix}\n",
    "\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\\\\n",
    "\\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}}\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "2. Compute $ \\mathbf{Q}^T \\mathbf{Q} $:\n",
    "\n",
    "$$ \\mathbf{Q}^T \\mathbf{Q} = \\begin{pmatrix}\n",
    "\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\\\\n",
    "\\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}}\n",
    "\\end{pmatrix} \\begin{pmatrix}\n",
    "\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\\\\n",
    "\\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}}\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "\\left(\\frac{1}{\\sqrt{2}}\\right)^2 + \\left(\\frac{1}{\\sqrt{2}}\\right)^2 & \\left(\\frac{1}{\\sqrt{2}}\\right)^2 - \\left(\\frac{1}{\\sqrt{2}}\\right)^2 \\\\\n",
    "\\left(\\frac{1}{\\sqrt{2}}\\right)^2 - \\left(\\frac{1}{\\sqrt{2}}\\right)^2 & \\left(\\frac{1}{\\sqrt{2}}\\right)^2 + \\left(\\frac{1}{\\sqrt{2}}\\right)^2\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{pmatrix} = \\mathbf{I} $$\n",
    "\n",
    "Since $ \\mathbf{Q}^T \\mathbf{Q} = \\mathbf{I} $, $ \\mathbf{Q} $ is indeed an orthogonal matrix.\n",
    "\n",
    "<u> Orthonormal Matrix </u>\n",
    "\n",
    "An orthonormal matrix is essentially another term for an orthogonal matrix. It emphasizes the fact that the columns (and rows) of the matrix are not only orthogonal but also normalized (i.e., they have unit length). Therefore, an orthonormal matrix has the same properties as an orthogonal matrix.\n",
    "\n",
    "<u> Differences Between Orthogonal and Orthonormal </u>\n",
    "\n",
    "The terms \"orthogonal matrix\" and \"orthonormal matrix\" are often used interchangeably because they refer to the same concept: a matrix whose columns and rows are orthonormal vectors. However, it's important to note that the distinction between orthogonal vectors and orthonormal vectors can be more general in other contexts:\n",
    "\n",
    "- **Orthogonal vectors**: Vectors that are perpendicular to each other (their dot product is zero).\n",
    "- **Orthonormal vectors**: Vectors that are orthogonal and of unit length.\n",
    "\n",
    "In summary, all orthonormal matrices are orthogonal, and in the context of matrices, the terms are used interchangeably.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
